import datetime
import pickle
import traceback
import hashlib
import os
import json
import collections
import subprocess
import copy

import util
from environment import Environment
from q_function import InverseLength, RandomQFunction

import torch
import wandb
from tqdm import tqdm
import numpy as np

from steps import Solution, Step


def _state_to_sol(state):
    states = state.facts
    actions = []
    action = state.parent_action
    while action is not None:
        actions.append(Step.from_string(action.action))
        action = action.state.parent_action
    actions = actions[::-1]
    return Solution(states, actions)


class SuccessRatePolicyEvaluator:
    """Evaluates the policy derived from a Q function by its success rate at solving
       problems generated by an environment."""
    def __init__(self, environment, config):
        self.environment = environment
        self.seed = config.get('seed', 0)
        self.n_problems = config.get('n_problems', 100)  # How many problems to use.
        self.max_steps = config.get('max_steps', 30)  # Maximum length of an episode.
        self.beam_size = config.get('beam_size', 1)  # Size of the beam in beam search.
        self.debug = config.get('debug', False)  # Whether to print all steps during evaluation.
        self.save_sols = config.get('save_sols')  # Whether to save solutions

    def evaluate(self, q, stored_solutions=None, verbose=False, show_progress=False):
        successes, failures, solution_lengths = [], [], []
        if self.save_sols:
            saved_sols = []
        wrapper = tqdm if show_progress else lambda x: x

        for i in wrapper(range(self.n_problems)):
            problem = self.environment.generate_new(seed=(self.seed + i))
            success, history = q.rollout(self.environment, problem,
                                         self.max_steps, self.beam_size, self.debug)
            if success:
                successes.append((i, problem))
                success_sol = history[-1][0]  # State object (backtrack along parent actions/states to get solution)
                if stored_solutions is not None:
                    stored_solutions.append(success_sol)
                if self.save_sols:
                    saved_sols.append(_state_to_sol(success_sol))  # save as Solution object
            else:
                failures.append((i, problem))
                if self.save_sols:
                    saved_sols.append(False)
            solution_lengths.append(len(history) - 1 if success else -1)
            if verbose:
                print(i, problem, '-- success?', success)

        np_sol_lens = np.array(solution_lengths)
        results = {
            'success_rate': len(successes) / self.n_problems,
            'solution_lengths': solution_lengths,
            'max_solution_length': max(solution_lengths),
            'mean_solution_length': np.mean(np_sol_lens[np_sol_lens >= 0]).item(),
            'successes': successes,
            'failures': failures
        }
        # if self.save_sols:
        #     results['solutions'] = saved_sols
        #     if isinstance(self.save_sols, str):
        #         with open(self.save_sols, 'wb') as f:
        #             pickle.dump(results, f)
        return results


class EndOfLearning(Exception):
    '''Exception used to signal the end of the learning budget for an agent.'''


class EnvironmentWithEvaluationProxy:
    '''Wrapper around the environment that triggers an evaluation every K calls'''
    def __init__(self, experiment_id: str, run_index: int, agent_name: str, domain: str,
            agent, environment: Environment, config: dict = {}, subrun_index=None, try_load_ckpt=True):
        self.config = config

        self.experiment_id = experiment_id
        self.run_index = run_index
        self.subrun_index = subrun_index  # for looping learning + abstracting
        self.agent_name = agent_name
        self.domain = domain
        self.environment = environment
        self.n_steps = 0

        self.evaluate_every = config.get('evaluate_every')
        self.eval_config = config['eval_config']
        self.agent = agent
        self.max_steps = config.get('max_steps')
        self.success_thres = config.get('success_thres')
        self.print_every = config.get('print_every', 100)

        self.results: list = []
        self.n_new_problems = 0
        self.cumulative_reward = 0
        self.begin_time = datetime.datetime.now()
        self.n_checkpoints = 0

        output_root = os.path.join(config['output_root'], experiment_id, agent_name, domain, f'run{run_index}')
        checkpoint_dir = os.path.join(output_root, 'checkpoints')

        os.makedirs(output_root, exist_ok=True)
        os.makedirs(checkpoint_dir, exist_ok=True)

        self.results_path = os.path.join(output_root, 'results.pkl')
        self.checkpoint_dir = checkpoint_dir

        if try_load_ckpt:
            self.load_checkpoint(config.get('restart_count', False))

    def load_checkpoint(self, restart_count=False):
        'Loads an existing training checkpoint, if available.'
        checkpoint_path = os.path.join(self.checkpoint_dir, 'training-state.pt')

        if os.path.exists(checkpoint_path):
            print('Training checkpoint exists - restoring...')
            device = self.agent.q_function.device
            checkpoint = torch.load(checkpoint_path, map_location=device)
            
            # Store agent's buffers that we don't want to replace
            ex_sols = self.agent.example_solutions
            new_max_negatives = self.agent.max_negatives
            stored_sols = self.agent.stored_solutions
            optimizer_state = self.agent.optimizer.state_dict() if self.agent.keep_optimizer else None
            
            if isinstance(checkpoint, dict) and 'agent_state' in checkpoint:
                # Restore Q-function weights
                self.agent.q_function.load_state_dict(checkpoint['agent_state']['q_function_state_dict'])
                
                # Restore agent state
                if not restart_count:
                    self.agent.training_problems_explored = checkpoint['agent_state']['training_problems_explored']
                    self.agent.training_problems_solved = checkpoint['agent_state']['training_problems_solved']
                    self.agent.training_acc_moving_average = checkpoint['agent_state']['training_acc_moving_average']
                    self.agent.current_depth = checkpoint['agent_state']['current_depth']
                    self.agent.bootstrapping = checkpoint['agent_state']['bootstrapping']

                    self.n_steps = checkpoint['n_steps']
                    self.n_new_problems = checkpoint['n_new_problems']
                    self.cumulative_reward = checkpoint['cumulative_reward']
                    self.n_checkpoints = checkpoint['n_checkpoints']
                    self.subrun_index = checkpoint['subrun_index']
                    
                    # Recreate environment with saved state
                    if 'environment_state' in checkpoint:
                        env_state = checkpoint['environment_state']
                        # Recreate environment with same configuration

                        env_config = {
                            'environment_backend': 'Rust' if hasattr(self.environment, 'rules') else 'Racket',
                            'domain': env_state['default_domain'],
                            'environment_url': self.config.get('environment_url', None),
                        }
                        if env_state['rules'] is not None:
                            env_config['abstractions'] = {'abs_ax': env_state['rules']}

                        self.environment = Environment.from_config(env_config)

                        if hasattr(self.environment, 'next_seed') and env_state['next_seed'] is not None:
                            self.environment.next_seed = env_state['next_seed']

            else:
                # Old format (entire object)
                saved_agent = checkpoint.agent
                self.agent.q_function.load_state_dict(saved_agent.q_function.state_dict())
                
                if not restart_count:
                    self.agent.training_problems_explored = saved_agent.training_problems_explored
                    self.agent.training_problems_solved = saved_agent.training_problems_solved
                    self.agent.training_acc_moving_average = saved_agent.training_acc_moving_average
                    self.agent.current_depth = saved_agent.current_depth
                    self.agent.bootstrapping = saved_agent.bootstrapping
                    
                    self.n_steps = checkpoint.n_steps
                    self.n_new_problems = checkpoint.n_new_problems
                    self.cumulative_reward = checkpoint.cumulative_reward
                    self.n_checkpoints = checkpoint.n_checkpoints
                    self.subrun_index = checkpoint.subrun_index
                    self.environment = checkpoint.environment


            self.agent.q_function.to(device)
            
            # Restore optimizer
            if not restart_count and optimizer_state and self.agent.keep_optimizer:
                self.agent.optimizer.load_state_dict(optimizer_state)
                
            if restart_count:
                self.agent.training_problems_explored = 0
                self.agent.training_problems_solved = 0
                self.agent.training_acc_moving_average = 0.0
                
            self.agent.example_solutions = ex_sols
            self.agent.stored_solutions = stored_sols
            self.agent.max_negatives = new_max_negatives

    def generate_new(self, domain=None, seed=None):
        self.n_new_problems += 1
        return self.environment.generate_new(domain, seed)

    def step(self, states, domain=None):
        n_steps_before = self.n_steps
        self.n_steps += len(states)

        # If the number of steps crossed the boundary of '0 mod evaluate_every', run evaluation.
        # If the agent took one step at a time, then we would only need to test if
        # n_steps % evaluate_every == 0. However the agent might take multiple steps at once.
        if self.agent.optimize_every is not None and (n_steps_before % self.evaluate_every) + len(states) >= self.evaluate_every:
            self.evaluate()

        if self.max_steps is not None and self.n_steps >= self.max_steps:
            # Budget ended.
            raise EndOfLearning()

        reward_and_actions = self.environment.step(states, domain)
        self.cumulative_reward += sum(rw for rw, _ in reward_and_actions)

        # Same logic as with evaluate_every.
        if self.agent.optimize_every is not None and (n_steps_before % self.print_every) + len(states) >= self.print_every:
            self.print_progress()

        return reward_and_actions

    def evaluate(self, final=False):
        print('Evaluating...')
        name, domain = self.agent_name, self.environment.default_domain

        self.environment.test()
        evaluator = SuccessRatePolicyEvaluator(self.environment, self.eval_config)
        results = evaluator.evaluate(self.agent.get_q_function())
        self.environment.train()
        results['n_steps'] = self.n_steps
        results['experiment_id'] = self.experiment_id
        results['run_index'] = self.run_index
        if self.subrun_index is not None:
            results['subrun_index'] = self.subrun_index
        results['name'] = name
        results['domain'] = domain
        results['problems_seen'] = self.n_new_problems
        results['cumulative_reward'] = self.cumulative_reward

        # wandb.log({'success_rate': results['success_rate'],
        #            'problems_seen': results['problems_seen'],
        #            'n_environment_steps': results['n_steps'],
        #            'cumulative_reward': results['cumulative_reward'],
        #            'max_solution_length': results['max_solution_length'],
        #            'mean_solution_length': results['mean_solution_length']
        #            })

        print(util.now(), f'Success rate ({name}-{domain}-run{self.run_index}):',
                results['success_rate'], '\tMax length:', results['max_solution_length'], '\tMean length:', results['mean_solution_length'])

        try:
            with open(self.results_path, 'rb') as f:
                existing_results = pickle.load(f)
        except Exception as e:
            print(f'Starting new results log at {self.results_path} ({e})')
            existing_results = []

        existing_results.append(results)

        with open(self.results_path, 'wb') as f:
            pickle.dump(existing_results, f)

        self.n_checkpoints += 1

        print("Saving checkpoint...")
        # Create a memory-efficient checkpoint
        checkpoint = {
            'agent_state': {
                'q_function_state_dict': self.agent.q_function.state_dict(),
                'training_problems_explored': self.agent.training_problems_explored,
                'training_problems_solved': self.agent.training_problems_solved,
                'training_acc_moving_average': self.agent.training_acc_moving_average,
                'current_depth': self.agent.current_depth,
                'bootstrapping': self.agent.bootstrapping,
            },
            'n_steps': self.n_steps,
            'n_new_problems': self.n_new_problems,
            'cumulative_reward': self.cumulative_reward,
            'n_checkpoints': self.n_checkpoints,
            'subrun_index': self.subrun_index,
            'environment_state': {
                'default_domain': self.environment.default_domain,
                'next_seed': getattr(self.environment, 'next_seed', None),
                'rules': getattr(self.environment, 'rules', None)
            }
        }
        torch.save(checkpoint, os.path.join(self.checkpoint_dir, 'training-state.pt'))
        print("Saving checkpoint completed.")

        if not final and (self.success_thres is not None and results['success_rate'] >= self.success_thres):
            raise EndOfLearning()


    def evaluate_agent(self):
        if self.n_checkpoints == 0:  # False when loading an existing training run
            self.evaluate()
        while True:
            try:
                self.agent.learn_from_environment(self)
            except EndOfLearning:
                print('Learning budget ended. Doing last learning round (if agent wants to)')
                self.agent.learn_from_experience(self)
                print('Running final evaluation...')
                self.evaluate(True)
                break
            except Exception as e:
                print('Exception during learning:', e)
                traceback.print_exc(e)
                print('Ignoring exception and continuing...')

    def print_progress(self):
        print(util.now(), '{} steps ({:.3}%, ETA: {}), {} total reward, explored {} problems. {}'
              .format(self.n_steps,
                      100 * (self.n_steps / self.max_steps) if self.max_steps is not None
                          else 100 * (self.agent.training_problems_solved / len(self.agent.example_solutions)),
                      util.format_eta(datetime.datetime.now() - self.begin_time,
                                      self.n_steps if self.max_steps is not None else self.agent.training_problems_solved,
                                      self.max_steps if self.max_steps is not None else len(self.agent.example_solutions)),
                      self.cumulative_reward,
                      self.n_new_problems,
                      self.agent.stats()))


def evaluate_policy(config, device, verbose):
    if config.get('random_policy'):
        q = RandomQFunction()
    elif config.get('inverse_length'):
        q = InverseLength()
    else:
        q = torch.load(config['model_path'], map_location=device)

    q.to(device)
    q.device = device

    env = Environment.from_config(config)
    evaluator = SuccessRatePolicyEvaluator(env, config.get('eval_config', {}))
    result = evaluator.evaluate(q, verbose=verbose, show_progress=True)

    if verbose:
        print('Success rate:', result['success_rate'])
        print('Max solution length:', result['max_solution_length'])
        print('Solved problems:', result['successes'])
        print('Unsolved problems:', result['failures'])

    return result['success_rate']


def evaluate_policy_checkpoints(config, device):
    previous_successes = set()
    checkpoint_path = config['checkpoint_path']
    env = Environment.from_config(config)
    evaluator = SuccessRatePolicyEvaluator(env, config.get('eval_config', {}))
    i = 0
    last_hash = None

    try:
        while True:
            path = checkpoint_path.format(i)
            i += 1

            with open(path, 'rb') as f:
                h = hashlib.md5(f.read()).hexdigest()
                if h == last_hash:
                    continue
                last_hash = h
            print('Evaluating', path)
            q = torch.load(path, map_location=device)
            q.to(device)
            q.device = device
            result = evaluator.evaluate(q, show_progress=True)

            for j, p in result['successes']:
                if j not in previous_successes:
                    print(f'New success: {j} :: {p.facts[-1]} (length: {result["solution_lengths"][j]})')

            for j, p in result['failures']:
                if j in previous_successes or i == 1:
                    print('New failure:', j, '::', p.facts[-1])

            previous_successes = set([j for j, _ in result['successes']])

            print('Success rate:', result['success_rate'])

    except FileNotFoundError:
        print('Checkpoint', i, 'does not exist -- stopping.')


def normalize_solutions(solutions: list[list[str]]) -> list[list[str]]:
    'Uses the Racket parser to syntactically normalize solutions in the equations domain.'
    all_steps = []

    for s in solutions:
        all_steps.extend(s)

    with open('input.txt', 'w') as f:
        for l in all_steps:
            f.write(l)
            f.write('\n')

    sp = subprocess.run(["racket", "-tm", "canonicalize-terms.rkt"], capture_output=True)
    steps = list(filter(None, sp.stdout.decode("utf8").split("\n")))

    new_solutions = []
    for s in solutions:
        new_solutions.append([steps.pop(0) for _ in range(len(s))])

    return new_solutions


def normalize_human_solutions(path):
    human_solutions = json.load(open(path))

    solutions = []

    for h in human_solutions:
        solutions.extend(h['solutions'])

    normalized_solutions = normalize_solutions(solutions)

    for h in human_solutions:
        for i in range(len(h['solutions'])):
            h['solutions'][i] = normalized_solutions.pop(0)

    with open('normalized_human_solutions.json', 'w') as f:
        json.dump(human_solutions, f)
